{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265db32f",
   "metadata": {},
   "source": [
    "## Siamese BERT-networks for semantic searching / information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d93f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "from random import sample, seed, shuffle\n",
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f2aca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.08751829713582993,\n",
       " 'start': 539,\n",
       " 'end': 585,\n",
       " 'answer': 'data scientist, start-up founder, and educator'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSON = 'Sinan Ozdemir'\n",
    "\n",
    "# Note this is NOT an efficient way to search on google. This is done simply for education purposes\n",
    "google_html = BeautifulSoup(requests.get(f'https://www.google.com/search?q={PERSON}').text).get_text()[:1024]\n",
    "\n",
    "nlp = pipeline('question-answering', \n",
    "               model='deepset/roberta-base-squad2', \n",
    "               tokenizer='deepset/roberta-base-squad2', \n",
    "               max_length=10)\n",
    "\n",
    "nlp(f'Who is {PERSON}?', google_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae22a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.12335775047540665,\n",
       " 'start': 268,\n",
       " 'end': 287,\n",
       " 'answer': '44th U.S. President'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSON = 'Barack Obama'\n",
    "\n",
    "# Note this is NOT an efficient way to search on google. This is done simply for education purposes\n",
    "google_html = BeautifulSoup(requests.get(f'https://www.google.com/search?q={PERSON}').text).get_text()[:1024]\n",
    "\n",
    "nlp = pipeline('question-answering', \n",
    "               model='deepset/roberta-base-squad2', \n",
    "               tokenizer='deepset/roberta-base-squad2', \n",
    "               max_length=10)\n",
    "\n",
    "nlp(f'Who is {PERSON}?', google_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b7963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e779f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 79 documents/paragraphs\n"
     ]
    }
   ],
   "source": [
    "# textbook about insects\n",
    "text = urlopen('https://www.gutenberg.org/cache/epub/10834/pg10834.txt').read().decode()\n",
    "\n",
    "# Only keep documents of at least 100 characters\n",
    "documents = list(filter(lambda x: len(x) > 100, text.split('\\r\\n\\r\\n')))\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "print(f'There are {len(documents)} documents/paragraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bc736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model pre-trained on an asymmetric semantic search task\n",
    "# We use the Bi-Encoder to encode all the documents, so that we can use it with sematic search\n",
    "bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "bi_encoder.max_seq_length = 256     # Truncate long documents to 256 tokens\n",
    "\n",
    "bi_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c7a87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5374875515454eaadce0362101a9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([79, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Documents are encoded by calling model.encode(). This takes about 5 minutes on my laptop\n",
    "document_embeddings = bi_encoder.encode(documents, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "document_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7b4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = 'How many horns does a flea have?'  # a natural language query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec83525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 14, 'score': 0.4899492859840393},\n",
       " {'corpus_id': 19, 'score': 0.24793782830238342},\n",
       " {'corpus_id': 21, 'score': 0.18478843569755554}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the query using the bi-encoder and find relevant documents\n",
    "question_embedding = bi_encoder.encode(QUESTION, convert_to_tensor=True)\n",
    "\n",
    "# Number of documents to retrieve with the bi-encoder\n",
    "hits = util.semantic_search(question_embedding, document_embeddings, top_k=3)[0]\n",
    "\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada5d037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many horns does a flea have?\n",
      "\n",
      "Document 1 Cos_Sim 0.490:\n",
      "\n",
      "When examined by a microscope, the flea is a pleasant object. The body\r\n",
      "is curiously adorned with a suit of polished armour, neatly jointed, and\r\n",
      "beset with a great number of sharp pins almost like the quills of a\r\n",
      "porcupine: it has a small head, large eyes, two horns, or feelers, which\r\n",
      "proceed from the head, and four long legs from the breast; they are very\r\n",
      "hairy and long, and have several joints, which fold as it were one\r\n",
      "within another.\n",
      "\n",
      "\n",
      "Document 2 Cos_Sim 0.248:\n",
      "\n",
      "The Chego is a very small animal, about one fourth the size of a common\r\n",
      "flea: it is very troublesome, in warm climates, to the poor blacks, such\r\n",
      "as go barefoot, and the slovenly: it penetrates the skin, under which it\r\n",
      "lays a bunch of eggs, which swell to the bigness of a small pea.\n",
      "\n",
      "\n",
      "Document 3 Cos_Sim 0.185:\n",
      "\n",
      "\r\n",
      "This is one of the largest of the insect tribe. It is met with in\r\n",
      "different countries, and of various sizes, from two or three inches to\r\n",
      "nearly a foot in length: it somewhat resembles a lobster, and casts its\r\n",
      "skin, as the lobster does its shell.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Question: {QUESTION}\\n')\n",
    "\n",
    "for i, hit in enumerate(hits):\n",
    "    \n",
    "    print(f'Document {i + 1} Cos_Sim {hit[\"score\"]:.3f}:\\n\\n{documents[hit[\"corpus_id\"]]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32cdde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8524739742279053, 'start': 259, 'end': 262, 'answer': 'two'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer the question from the top document\n",
    "nlp(QUESTION, str(documents[hits[0]['corpus_id']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901b68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is called an \"Open Book Q/A\" System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc6128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f376c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (/Users/sinanozdemir/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    }
   ],
   "source": [
    "# load up the adversarial_qa dataset from the Q/A use-case\n",
    "training_qa = load_dataset('adversarial_qa', 'adversarialQA', split='train')\n",
    "\n",
    "good_training_data = []\n",
    "bad_training_data = []\n",
    "    \n",
    "last_example = None\n",
    "for example in training_qa:\n",
    "    if last_example and example['context'] != last_example['context']:\n",
    "        bad_training_data.append((example['question'], last_example['context'], 0.0))  #  add neutral examples\n",
    "    # question, context, label is 1 if should be matched together\n",
    "    good_training_data.append((example['question'], example['context'], 1.0))\n",
    "    last_example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ca4c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2647)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_training_data), len(bad_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "379d8a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What letter designates what Ektachrome is designed for?',\n",
       " 'Some high-speed black-and-white films, such as Ilford Delta 3200 and Kodak T-MAX P3200, are marketed with film speeds in excess of their true ISO speed as determined using the ISO testing method. For example, the Ilford product is actually an ISO 1000 film, according to its data sheet. The manufacturers do not indicate that the 3200 number is an ISO rating on their packaging. Kodak and Fuji also marketed E6 films designed for pushing (hence the \"P\" prefix), such as Ektachrome P800/1600 and Fujichrome P1600, both with a base speed of ISO 400.',\n",
       " 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_training_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "920aa0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What film beside Ektachrome and Fujichorme is designed for pushing?',\n",
       " 'The Weston Cadet (model 852 introduced in 1949), Direct Reading (model 853 introduced 1954) and Master III (models 737 and S141.3 introduced in 1956) were the first in their line of exposure meters to switch and utilize the meanwhile established ASA scale instead. Other models used the original Weston scale up until ca. 1955. The company continued to publish Weston film ratings after 1955, but while their recommended values often differed slightly from the ASA film speeds found on film boxes, these newer Weston values were based on the ASA system and had to be converted for use with older Weston meters by subtracting 1/3 exposure stop as per Weston\\'s recommendation. Vice versa, \"old\" Weston film speed ratings could be converted into \"new\" Westons and the ASA scale by adding the same amount, that is, a film rating of 100 Weston (up to 1955) corresponded with 125 ASA (as per ASA PH2.5-1954 and before). This conversion was not necessary on Weston meters manufactured and Weston film ratings published since 1956 due to their inherent use of the ASA system; however the changes of the ASA PH2.5-1960 revision may be taken into account when comparing with newer ASA or ISO values.',\n",
       " 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_training_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "441fddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/training/overview.html for more information on training\n",
    "\n",
    "seed(42)  # seed our upcoming sample\n",
    "\n",
    "sampled_training_data = sample(good_training_data, 1000) + sample(bad_training_data, 1000)\n",
    "\n",
    "shuffle(sampled_training_data)  # shuffle our data around\n",
    "\n",
    "training_index = int(.8 * len(sampled_training_data))  # Get an 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067c6bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': '',\n",
       " 'texts': (\"Why might I question Tito's ethics?\",\n",
       "  'In 2013 a lot of media coverage was given to unclassified NSA\\'s study in Cryptologic Spectrum that concluded that Tito did not speak the language as a native, and had features of other Slavic languages (Russian and Polish). The hypothesis that \"a non-Yugoslav, perhaps a Russian or a Pole\" assumed Tito\\'s identity was included. The report also notes Draža Mihailović\\'s impressions of Tito\\'s Russian origins.'),\n",
       " 'label': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the training examples\n",
    "train_examples = [InputExample(texts=t[:2], label=t[2]) for t in sampled_training_data[:training_index]]\n",
    "\n",
    "train_examples[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d22f57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train dataset, a dataloader and the train loss\n",
    "# A data loader is the object that specifically shuffles/grabs batches of data from a Dataset\n",
    "# We don't usually have to explicitly create one using the Trainer because it has a default loader built in\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(bi_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e306f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2af814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation data, sentences1 and sentences2 are lists of questions and context respectively and scores are 0 or 1\n",
    "sentences1, sentences2, scores = zip(*sampled_training_data[training_index:])\n",
    "\n",
    "# evaluator will evaluate embedding closeness\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9343a870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4705443607475632"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_encoder.evaluate(evaluator)  # initial evalaution (higher embedding similarity is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8269f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinanozdemir/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7530468c14440580c9edaf4ae570eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a219889d4b1d4fcc8b7a2a1c4bed179b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9bb3bbf0a448889fb7f64d05b3cb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fine-tune the model using the fit method\n",
    "bi_encoder.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)], \n",
    "    output_path='ir/results',\n",
    "    epochs=2, \n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24b628e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4726230620789223"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_encoder.evaluate(evaluator)  # final evalaution (higher embedding similarity is better)\n",
    "# Not a huge jump in performance with 2 epochs. We could try more data or more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cefe5dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load fine-tuned IR model\n",
    "finetuned_bi_encoder = SentenceTransformer('ir/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b62e97a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66bb529f71545f2a00ea04b10066d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many horns does a flea have?\n",
      "\n",
      "Document 1 Cos_Sim 0.496:\n",
      "\n",
      "When examined by a microscope, the flea is a pleasant object. The body\r\n",
      "is curiously adorned with a suit of polished armour, neatly jointed, and\r\n",
      "beset with a great number of sharp pins almost like the quills of a\r\n",
      "porcupine: it has a small head, large eyes, two horns, or feelers, which\r\n",
      "proceed from the head, and four long legs from the breast; they are very\r\n",
      "hairy and long, and have several joints, which fold as it were one\r\n",
      "within another.\n",
      "\n",
      "\n",
      "Document 2 Cos_Sim 0.255:\n",
      "\n",
      "The Chego is a very small animal, about one fourth the size of a common\r\n",
      "flea: it is very troublesome, in warm climates, to the poor blacks, such\r\n",
      "as go barefoot, and the slovenly: it penetrates the skin, under which it\r\n",
      "lays a bunch of eggs, which swell to the bigness of a small pea.\n",
      "\n",
      "\n",
      "Document 3 Cos_Sim 0.192:\n",
      "\n",
      "\r\n",
      "This is one of the largest of the insect tribe. It is met with in\r\n",
      "different countries, and of various sizes, from two or three inches to\r\n",
      "nearly a foot in length: it somewhat resembles a lobster, and casts its\r\n",
      "skin, as the lobster does its shell.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Slightly more confident results!\n",
    "\n",
    "document_embeddings = finetuned_bi_encoder.encode(documents, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "question_embedding = finetuned_bi_encoder.encode(QUESTION, convert_to_tensor=True)\n",
    "\n",
    "# Get document hits\n",
    "hits = util.semantic_search(question_embedding, document_embeddings, top_k=3)[0]\n",
    "\n",
    "print(f'Question: {QUESTION}\\n')\n",
    "\n",
    "for i, hit in enumerate(hits):\n",
    "    \n",
    "    print(f'Document {i + 1} Cos_Sim {hit[\"score\"]:.3f}:\\n\\n{documents[hit[\"corpus_id\"]]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7585813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gutenberg_to_documents(gutenberg_url, bi_encoder):\n",
    "    text = urlopen(gutenberg_url).read().decode()\n",
    "    documents = np.array(list(filter(lambda x: len(x) > 100, text.split('\\r\\n\\r\\n'))))\n",
    "    print(f'There are {len(documents)} documents/paragraphs')\n",
    "    return documents, bi_encoder.encode(documents)\n",
    "\n",
    "def retrieve_relevant_documents(bi_encoder, query, documents, document_embeddings, hits=3):\n",
    "    query_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    hits = util.semantic_search(query_embedding, document_embeddings, top_k=hits)[0]\n",
    "\n",
    "    for i, hit in enumerate(hits):\n",
    "        print(f'Document {i + 1} Cos_Sim {hit[\"score\"]:.3f}:\\n\\n{documents[hit[\"corpus_id\"]]}')\n",
    "        print('\\n')\n",
    "    print(f\"Answer from Top Document: {nlp(query, str(documents[hits[0]['corpus_id']]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc99564c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1402 documents/paragraphs\n"
     ]
    }
   ],
   "source": [
    "banks_to_bassoon_documents, banks_to_bassoon_embeddings = gutenberg_to_documents(\n",
    "    'https://www.gutenberg.org/cache/epub/27480/pg27480.txt', finetuned_bi_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d5a447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Cos_Sim 0.754:\n",
      "\n",
      "BANSHEE (Irish _bean sidhe_; Gaelic _ban sith_, \"woman of the fairies\"), a\n",
      "supernatural being in Irish and general Celtic folklore, whose mournful\n",
      "screaming, or \"keening,\" at night is held to foretell the death of some\n",
      "member of the household visited. In Ireland legends of the banshee belong\n",
      "more particularly to certain families in whose records periodic visits from\n",
      "the spirit are chronicled. A like ghostly informer figures in Brittany\n",
      "folklore. The Irish banshee is held to be the distinction only of families\n",
      "of pure Milesian descent. The Welsh have the banshee under the name _gwrach\n",
      "y Rhibyn_ (witch of Rhibyn). Sir Walter Scott mentions a belief in the\n",
      "banshee as existing in the highlands of Scotland (_Demonology and\n",
      "Witchcraft_, p. 351). A Welsh death-portent often confused with the gwrach\n",
      "y Rhibyn and banshee is the _cyhyraeth_, the groaning spirit.\n",
      "\n",
      "\n",
      "Document 2 Cos_Sim 0.325:\n",
      "\n",
      "BANNU, a town and district of British India, in the Derajat division of the\n",
      "North-West Frontier Province. The town (also called Edwardesabad and\n",
      "Dhulipnagar) lies in the north-west corner of the district, in the valley\n",
      "of the Kurram river. Pop. (1901) 14,300. It forms the base for all punitive\n",
      "expeditions to the Tochi Valley and Waziri frontier.\n",
      "\n",
      "\n",
      "Answer from Top Document: {'score': 0.044728901237249374, 'start': 76, 'end': 94, 'answer': 'supernatural being'}\n"
     ]
    }
   ],
   "source": [
    "retrieve_relevant_documents(finetuned_bi_encoder,\n",
    "    'What is a banshee?', banks_to_bassoon_documents, banks_to_bassoon_embeddings, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f3931a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Cos_Sim 0.800:\n",
      "\n",
      "[3] The date 1876 is taken as being that when the Imperial Bank of Germany\n",
      "came into full operation.\n",
      "\n",
      "\n",
      "Document 2 Cos_Sim 0.581:\n",
      "\n",
      "Similar banks had been established in Middelburg, (March 28th, 1616), in\n",
      "Hamburg (1619) and in Rotterdam (February 9th, 1635). Of these the Bank of\n",
      "Hamburg carried on much the largest business and survived the longest. It\n",
      "was not till the 15th of February 1873 that its existence was closed by the\n",
      "act of the German parliament which decreed that Germany should possess a\n",
      "gold standard, and thus removed those conditions of the local medium of\n",
      "exchange--silver coins of very different intrinsic values--whose\n",
      "circulation had provided an ample field for the operations of the bank. The\n",
      "business of the Bank of Hamburg had been conducted in absolute accordance\n",
      "with the regulations under which it was founded.\n",
      "\n",
      "\n",
      "Answer from Top Document: {'score': 0.1893422156572342, 'start': 13, 'end': 17, 'answer': '1876'}\n"
     ]
    }
   ],
   "source": [
    "retrieve_relevant_documents(finetuned_bi_encoder,\n",
    "    'When was the Imperial Bank of Germany founded?', banks_to_bassoon_documents, banks_to_bassoon_embeddings, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361af40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc698cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655970a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
