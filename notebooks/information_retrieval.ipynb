{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "homeless-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 documents\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "\n",
    "response = urlopen('''https://www.gutenberg.org/cache/epub/10834/pg10834.txt''')  # insects\n",
    "\n",
    "PARAGRAPH_SPLITTER = '\\r\\n\\r\\n'\n",
    "\n",
    "text = response.read().decode()\n",
    "\n",
    "text = text[text.index(\"***START OF THE PROJECT GUTENBERG\") :text.index(\"***END OF THE PROJECT GUTENBERG\")]\n",
    "\n",
    "documents = text.split(PARAGRAPH_SPLITTER)\n",
    "\n",
    "\n",
    "documents = list(filter(lambda x: len(x) > 25, documents))\n",
    "\n",
    "def preprocess(s):\n",
    "    return s.strip()\n",
    "\n",
    "documents =  list(map(preprocess, documents))\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "print(f'There are {len(documents)} documents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "arbitrary-baseball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"This cut shews the appearance of the worm, which at first is very small\\r\\nand black. Its food is the leaves of the white mulberry: as it grows in\\r\\nsize, at four different periods, it apparently sickens, and changes its\\r\\nskin, and finally, when full grown, it spins a ball of silk, called a\\r\\ncone, or cocoon, the thread of which is about three hundred yards long:\\r\\nin the centre of this ball the worm entombs itself, and experiences a\\r\\nchange to a state called an aurelia, or chrysallis, as seen below the\\r\\nball: from this aurelia, the moth that lays the eggs is hatched, and\\r\\nthus goes on the round of this animal's changes, or transmigrations.\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample, seed\n",
    "\n",
    "seed(42)\n",
    "\n",
    "sample(sorted(documents), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "former-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# a model pre-trained on an asymmetric semantic search task\n",
    "sbert_model = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "\n",
    "# Documents are encoded by calling model.encode()\n",
    "document_embeddings = sbert_model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "coordinate-indiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "expensive-fluid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40406528, -0.16512644, -0.0930415 , -0.1626442 , -0.6941118 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model.encode(['hi'])[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "quick-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = 'How many horns does a flea have?'  # a natural language query\n",
    "\n",
    "query_embedding = sbert_model.encode(QUESTION)  # embed the query into a vector space\n",
    "\n",
    "top_scores = util.cos_sim(query_embedding, document_embeddings)  # use cosine similarity to find the most relevant document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "obvious-score",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[-0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.0934, -0.0648, -0.0596,\n",
       "         -0.0552, -0.0471, -0.0456, -0.0452, -0.0336, -0.0274, -0.0118, -0.0095,\n",
       "         -0.0078, -0.0030,  0.0079,  0.0102,  0.0112,  0.0112,  0.0134,  0.0149,\n",
       "          0.0199,  0.0208,  0.0208,  0.0243,  0.0305,  0.0335,  0.0395,  0.0400,\n",
       "          0.0468,  0.0474,  0.0489,  0.0566,  0.0592,  0.0613,  0.0665,  0.0675,\n",
       "          0.0969,  0.1088,  0.1099,  0.1182,  0.1272,  0.1417,  0.1619,  0.1848,\n",
       "          0.2479,  0.4899]]),\n",
       "indices=tensor([[ 6, 10, 15,  8, 12, 13, 44, 47, 46, 48, 43,  2,  3, 22, 14, 21, 37, 19,\n",
       "         18,  1,  9, 11,  4, 49, 27,  7,  5, 25, 35, 36,  0, 39, 45, 38, 23, 30,\n",
       "         20, 16, 17, 42, 40, 29, 24, 41, 34, 32, 28, 33, 31, 26]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_scores.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "constant-vitamin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Document 1 Cos_Sim 0.490:\n",
      "\n",
      "When examined by a microscope, the flea is a pleasant object. The body\r\n",
      "is curiously adorned with a suit of polished armour, neatly jointed, and\r\n",
      "beset with a great number of sharp pins almost like the quills of a\r\n",
      "porcupine: it has a small head, large eyes, two horns, or feelers, which\r\n",
      "proceed from the head, and four long legs from the breast; they are very\r\n",
      "hairy and long, and have several joints, which fold as it were one\r\n",
      "within another.\n",
      "\n",
      "\n",
      "Top Document 2 Cos_Sim 0.248:\n",
      "\n",
      "The Chego is a very small animal, about one fourth the size of a common\r\n",
      "flea: it is very troublesome, in warm climates, to the poor blacks, such\r\n",
      "as go barefoot, and the slovenly: it penetrates the skin, under which it\r\n",
      "lays a bunch of eggs, which swell to the bigness of a small pea.\n",
      "\n",
      "\n",
      "Top Document 3 Cos_Sim 0.185:\n",
      "\n",
      "This is one of the largest of the insect tribe. It is met with in\r\n",
      "different countries, and of various sizes, from two or three inches to\r\n",
      "nearly a foot in length: it somewhat resembles a lobster, and casts its\r\n",
      "skin, as the lobster does its shell.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_documents = documents[top_scores.sort().indices[0][-3:]][::-1]\n",
    "top_cosine_sim = list(top_scores.sort().values[0][-3:])[::-1]\n",
    "\n",
    "for i, (cos_sim, top_document) in enumerate(zip(top_cosine_sim, top_documents)):\n",
    "    print(f'Top Document {i + 1} Cos_Sim {cos_sim:.3f}:\\n\\n{top_document}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "baking-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained('bert-base-cased')  # distilbert doesn't have token type IDs\n",
    "qa_bert = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-toilet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hairy-fighter",
   "metadata": {},
   "source": [
    "# Using a pretrained model to fine-tune our answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "constitutional-manual",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad', tokenizer='distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fluid-fireplace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When examined by a microscope, the flea is a pleasant object. The body\\r\\nis curiously adorned with a suit of polished armour, neatly jointed, and\\r\\nbeset with a great number of sharp pins almost like the quills of a\\r\\nporcupine: it has a small head, large eyes, two horns, or feelers, which\\r\\nproceed from the head, and four long legs from the breast; they are very\\r\\nhairy and long, and have several joints, which fold as it were one\\r\\nwithin another.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "excessive-supplement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6974557638168335, 'start': 259, 'end': 262, 'answer': 'two'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=QUESTION, context=top_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-powell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decent-import",
   "metadata": {},
   "source": [
    "# Using our pretrained model from the last use-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "hybrid-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "qa_bert_finetuned = BertForQuestionAnswering.from_pretrained('./qa/results')\n",
    "\n",
    "finetuned_qa = pipeline('question-answering', model=qa_bert_finetuned, tokenizer='bert-large-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "rocky-polish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.01910480484366417, 'start': 259, 'end': 262, 'answer': 'two'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_qa(question=QUESTION, context=top_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-capture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "identical-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/pretrained_models.html for more fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-chain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "divided-radius",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (/Users/sinanozdemir/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load up the adversarial_qa dataset from the last use-case\n",
    "training_qa = load_dataset('adversarial_qa', 'adversarialQA', split='train')\n",
    "\n",
    "good_training_data = []\n",
    "bad_training_data = []\n",
    "    \n",
    "last_example = None\n",
    "for example in training_qa:\n",
    "    if last_example and example['context'] != last_example['context']:\n",
    "        bad_training_data.append((example['question'], last_example['context'], 0.0))  #  add bad examples\n",
    "    # question, context, label is 1 for these should be matched together\n",
    "    good_training_data.append((example['question'], example['context'], 1.0))\n",
    "    last_example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "convinced-terrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2647)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_training_data), len(bad_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "stone-reproduction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/training/overview.html for information on training\n",
    "\n",
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from random import sample, seed, shuffle\n",
    "\n",
    "seed(42)  # seed our upcoming sample\n",
    "\n",
    "sampled_training_data = sample(good_training_data, 500) + sample(bad_training_data, 500)\n",
    "\n",
    "shuffle(sampled_training_data)\n",
    "\n",
    "\n",
    "#Define the training examples\n",
    "train_examples = [InputExample(texts=t[:2], label=t[2]) for t in sampled_training_data[:800]]\n",
    "\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "train_loss = losses.CosineSimilarityLoss(sbert_model)\n",
    "\n",
    "# Evaluation data\n",
    "sentences1, sentences2, scores = zip(*sampled_training_data[800:])\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "engaging-diagram",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0536f798c35748ca9bfd03b5c9fe593a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5512e204c81f422c9bee6df5f3715efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc3662dbe52450bb99b6d51b4266f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tune the model\n",
    "sbert_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)], output_path='ir/results',\n",
    "    epochs=2, warmup_steps=50, \n",
    "    evaluator=evaluator, evaluation_steps=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-austria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-office",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aboriginal-amount",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2940645 , -0.15025584, -0.1537622 , -0.15683304, -0.74979   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load fine-tuned IR model\n",
    "finetuned_sbert_model = SentenceTransformer('ir/results')\n",
    "\n",
    "finetuned_sbert_model.encode(['hi'])[0][:5]  # different embedding as before which is expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-bundle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "roman-inside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Document 1 Cos_Sim 0.600:\n",
      "\n",
      "When examined by a microscope, the flea is a pleasant object. The body\r\n",
      "is curiously adorned with a suit of polished armour, neatly jointed, and\r\n",
      "beset with a great number of sharp pins almost like the quills of a\r\n",
      "porcupine: it has a small head, large eyes, two horns, or feelers, which\r\n",
      "proceed from the head, and four long legs from the breast; they are very\r\n",
      "hairy and long, and have several joints, which fold as it were one\r\n",
      "within another.\n",
      "\n",
      "\n",
      "Top Document 2 Cos_Sim 0.393:\n",
      "\n",
      "The Chego is a very small animal, about one fourth the size of a common\r\n",
      "flea: it is very troublesome, in warm climates, to the poor blacks, such\r\n",
      "as go barefoot, and the slovenly: it penetrates the skin, under which it\r\n",
      "lays a bunch of eggs, which swell to the bigness of a small pea.\n",
      "\n",
      "\n",
      "Top Document 3 Cos_Sim 0.348:\n",
      "\n",
      "In examining the louse with a microscope, its external deformity strikes\r\n",
      "us with disgust. It has six feet, two eyes, and a sort of sting,\r\n",
      "proboscis, or sucker, with which it pierces the skin, and sucks the\r\n",
      "blood. The skin of the louse is hard and transparent, with here and\r\n",
      "there several bristly hairs: at the end of each leg are two claws, by\r\n",
      "which it is enabled to lay hold of the hairs, on which it climbs. There\r\n",
      "is scarcely any animal known to multiply so fast as this unwelcome\r\n",
      "intruder: from an experiment of Lieuenhoek, a louse in eight weeks, may\r\n",
      "see five thousand of its descendants.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# re-encode the documents and run the same question as before\n",
    "document_embeddings = sbert_model.encode(documents)\n",
    "\n",
    "query_embedding = sbert_model.encode(QUESTION)  # embed the query into a vector space\n",
    "\n",
    "top_scores = util.cos_sim(query_embedding, document_embeddings)  # use cosine similarity to find the most relevant document\n",
    "\n",
    "top_documents = documents[top_scores.sort().indices[0][-3:]][::-1]\n",
    "top_cosine_sim = list(top_scores.sort().values[0][-3:])[::-1]\n",
    "\n",
    "for i, (cos_sim, top_document) in enumerate(zip(top_cosine_sim, top_documents)):\n",
    "    print(f'Top Document {i + 1} Cos_Sim {cos_sim:.3f}:\\n\\n{top_document}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-outreach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "built-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gutenberg_to_documents(url, sbert_model):\n",
    "    response = urlopen(url)\n",
    "\n",
    "    PARAGRAPH_SPLITTER = '\\r\\n\\r\\n'\n",
    "\n",
    "    text = response.read().decode()\n",
    "    try:\n",
    "        text = text[text.index(\"***START OF THE PROJECT GUTENBERG\") :text.index(\"***END OF THE PROJECT GUTENBERG\")]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    documents = text.split(PARAGRAPH_SPLITTER)\n",
    "\n",
    "    documents = list(filter(lambda x: len(x) > 25, documents))\n",
    "\n",
    "    def preprocess(s):\n",
    "        return s.strip()\n",
    "\n",
    "    documents =  list(map(preprocess, documents))\n",
    "\n",
    "    documents = np.array(documents)\n",
    "\n",
    "    print(f'There are {len(documents)} documents')\n",
    "    \n",
    "    return documents, sbert_model.encode(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "comparative-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(sbert_model, query, documents, embeddings, qa=None):\n",
    "    query_embedding = sbert_model.encode(query)  # embed the query into a vector space\n",
    "\n",
    "    top_scores = util.cos_sim(query_embedding, embeddings)  # use cosine similarity to find the most relevant document\n",
    "    top_documents = documents[top_scores.sort().indices[0][-3:]][::-1]\n",
    "    top_cosine_sim = list(top_scores.sort().values[0][-3:])[::-1]\n",
    "\n",
    "    for i, (cos_sim, top_document) in enumerate(zip(top_cosine_sim, top_documents)):\n",
    "        print(f'Top Document {i + 1} Cos_Sim {cos_sim:.3f}:\\n\\n{top_document}')\n",
    "        if qa:\n",
    "            answer = qa(question=query, context=top_document)\n",
    "            print(f'\\nAnswer: {answer}\\n')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-evolution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "portable-screen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1599 documents\n"
     ]
    }
   ],
   "source": [
    "banks_to_bassoon_documents, banks_to_bassoon_embeddings = gutenberg_to_documents(\n",
    "    'https://www.gutenberg.org/cache/epub/27480/pg27480.txt', finetuned_sbert_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "powered-monthly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Document 1 Cos_Sim 0.761:\n",
      "\n",
      "BANSHEE (Irish _bean sidhe_; Gaelic _ban sith_, \"woman of the fairies\"), a\n",
      "supernatural being in Irish and general Celtic folklore, whose mournful\n",
      "screaming, or \"keening,\" at night is held to foretell the death of some\n",
      "member of the household visited. In Ireland legends of the banshee belong\n",
      "more particularly to certain families in whose records periodic visits from\n",
      "the spirit are chronicled. A like ghostly informer figures in Brittany\n",
      "folklore. The Irish banshee is held to be the distinction only of families\n",
      "of pure Milesian descent. The Welsh have the banshee under the name _gwrach\n",
      "y Rhibyn_ (witch of Rhibyn). Sir Walter Scott mentions a belief in the\n",
      "banshee as existing in the highlands of Scotland (_Demonology and\n",
      "Witchcraft_, p. 351). A Welsh death-portent often confused with the gwrach\n",
      "y Rhibyn and banshee is the _cyhyraeth_, the groaning spirit.\n",
      "\n",
      "Answer: {'score': 0.1614086925983429, 'start': 73, 'end': 94, 'answer': 'a\\r\\nsupernatural being'}\n",
      "\n",
      "\n",
      "\n",
      "Top Document 2 Cos_Sim 0.399:\n",
      "\n",
      "BANYAN, or BANIAN (an Arab corruption, borrowed by the Portuguese from the\n",
      "Sanskrit _vanij_, \"merchant\"), the _Ficus Indica_, or _Bengalensis_, a tree\n",
      "of the fig genus. The name was originally given by Europeans to a\n",
      "particular tree on the Persian Gulf beneath which some Hindu \"merchants\"\n",
      "had built a pagoda. In Calcutta the word was once generally applied to a\n",
      "native broker or head clerk in any business or private house, now usually\n",
      "known as sircar. _Bunya_, a corruption of the word common in Bengal\n",
      "generally, is usually applied to the native grain-dealer. Early writers\n",
      "sometimes use the term generically for all Hindus in western India.\n",
      "_Banyan_ was long Anglo-Indian for an undershirt, in allusion to the body\n",
      "garment of the Hindus, especially the Banyans.\n",
      "\n",
      "Answer: {'score': 0.051671646535396576, 'start': 556, 'end': 568, 'answer': 'grain-dealer'}\n",
      "\n",
      "\n",
      "\n",
      "Top Document 3 Cos_Sim 0.379:\n",
      "\n",
      "BANNU, a town and district of British India, in the Derajat division of the\n",
      "North-West Frontier Province. The town (also called Edwardesabad and\n",
      "Dhulipnagar) lies in the north-west corner of the district, in the valley\n",
      "of the Kurram river. Pop. (1901) 14,300. It forms the base for all punitive\n",
      "expeditions to the Tochi Valley and Waziri frontier.\n",
      "\n",
      "Answer: {'score': 0.3814353346824646, 'start': 7, 'end': 43, 'answer': 'a town and district of British India'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieve_relevant_documents(finetuned_sbert_model,\n",
    "    'What is a banshee?', banks_to_bassoon_documents, banks_to_bassoon_embeddings,\n",
    "    qa=qa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "studied-liechtenstein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Document 1 Cos_Sim 0.563:\n",
      "\n",
      "Amateur base-ball, in its organized phase, is played mostly by school and\n",
      "university clubs as well as those of athletic associations. The first\n",
      "college league was formed in 1879 and comprised Harvard, Princeton,\n",
      "Amherst, Brown and Dartmouth, Yale joining a year later. The Eastern\n",
      "College League, with Columbia, Harvard, Princeton and Yale, followed in\n",
      "1887. This was afterwards dissolved and at present the most important\n",
      "universities of the eastern states are members of no league, although such\n",
      "organizations exist in New England and different parts of the west and\n",
      "south. Amateur base-ball has progressed along the same lines as\n",
      "professional, although the college playing rules formerly differed in\n",
      "certain minor points from those of the professional leagues.\n",
      "\n",
      "Answer: {'score': 0.48191583156585693, 'start': 63, 'end': 91, 'answer': 'school and\\r\\nuniversity clubs'}\n",
      "\n",
      "\n",
      "\n",
      "Top Document 2 Cos_Sim 0.498:\n",
      "\n",
      "BASE-BALL (so-called from the bases and ball used), the national summer\n",
      "sport of the United States, popular also throughout Canada and in Japan.\n",
      "Its origin is obscure. According to some authorities it is derived from the\n",
      "old English game of rounders (_q.v._), several variations of which were\n",
      "played in America during the colonial period; according to other\n",
      "authorities, its resemblance to rounders is merely a coincidence, and it\n",
      "had its origin in the United States, probably at Cooperstown, New York, in\n",
      "1839, when it is said, Abner Doubleday (later a general in the U.S. army)\n",
      "devised a scheme for playing it. About the beginning of the 19th century a\n",
      "game generally known as \"One Old Cat\" became popular with schoolboys in the\n",
      "North Atlantic states; this game was played by three boys, each fielding\n",
      "and batting in turn, a run being scored by the batsman running to a single\n",
      "base and back without being put out. Two Old Cat, Three Old Cat, and Four\n",
      "Old Cat were modifications of this game, having respectively four, six, and\n",
      "eight players. A development of this game bore the name of town-ball and\n",
      "the Olympic Town-Ball Club of Philadelphia was organized in 1833. Matches\n",
      "between organized base-ball clubs were first played in the neighbourhood of\n",
      "New York, where the Washington Baseball Club was founded in 1843. The first\n",
      "regular code of rules was drawn up in 1845 by the Knickerbocker Baseball\n",
      "Club and used in its matches with the Gotham Eagle and Empire clubs of New\n",
      "York, and the Excelsior, Putnam, Atlantic and Eckford clubs of Brooklyn. In\n",
      "1858 the first National Association was organized, and, while its few\n",
      "simple laws were generally similar to the corresponding rules of the\n",
      "present code, the ball was larger and \"livelier,\" and the pitcher was\n",
      "compelled to deliver it with a full toss, no approach to a throw being\n",
      "allowed. The popularity of the game spread rapidly, resulting in the\n",
      "organization of many famous clubs, such as the Beacon and Lowell of Boston,\n",
      "the Red Stockings of Cincinnati, the Forest City of Cleveland and the Maple\n",
      "Leaf of Guelph, but owing to the sharp rivalry between the foremost teams,\n",
      "semi-professionalism soon crept in, although in those days a man who played\n",
      "for a financial consideration always had some other means of livelihood, as\n",
      "the income to be derived from playing ball in the summer time was not\n",
      "enough to support him throughout the year. In spite of its popularity, the\n",
      "game acquired certain undesirable adjuncts. The betting and pool selling\n",
      "evils became prominent, and before long the game was in thorough disrepute.\n",
      "It was not only generally believed that the matches were not played on\n",
      "their merits, but it was known that players themselves were not above\n",
      "selling contests. At that time many of the journals of the day foretold the\n",
      "speedy downfall of the sport. A convention of those interested financially\n",
      "and otherwise in the game, was held in 1867 in Philadelphia, and an effort\n",
      "was made to effect a reformation. That the sport even then was by no means\n",
      "insignificant can be seen from the fact that in that convention some 500\n",
      "organizations were represented. While the work done at the convention did\n",
      "not accomplish all that was expected, it did produce certain reforms, and\n",
      "the sport grew rapidly thereafter both in the eastern and in the middle\n",
      "western part of the United States. In the next five years the [v.03 p.0459]\n",
      "interest in the game became so great that it was decided to send a\n",
      "representation of American base-ball players to England; and two clubs, the\n",
      "Bostons, who were the champions that year, and the Athletics, former\n",
      "champions, crossed the Atlantic and played several exhibition games with\n",
      "each other. While successful in exciting some interest, the trip did not\n",
      "succeed in popularizing base-ball in Great Britain. Fifteen years later two\n",
      "other nines of representative American base-ball players made a general\n",
      "tour of Australia and various other countries, completing their trip by a\n",
      "contest in England. This too, however, had little effect, and later\n",
      "attempts to establish base-ball in England have likewise been unsuccessful.\n",
      "But in America the game continued to prosper. The first entirely\n",
      "professional club was the Cincinnati Red Stockings (1868). Two national\n",
      "associations were formed in 1871, one having jurisdiction over professional\n",
      "clubs and the other over amateurs. In 1876 was formed the National League,\n",
      "of eight clubs under the presidency of Nicholas E. Young, which contained\n",
      "the expert ball-players of the country. There were so many people in the\n",
      "United States who wanted to see professional base-ball that this\n",
      "organization proved too small to furnish the desired number of games, and\n",
      "hence in 1882 the American Association was formed. For a time it seemed\n",
      "that there would be room for both organizations; but there was considerable\n",
      "rivalry, and it was not until an agreement was made between the two\n",
      "organizations that they were able to work together in harmony. They\n",
      "practically controlled professional base-ball for many years, although\n",
      "there were occasional attempts to overthrow their authority, the most\n",
      "notable being the formation in 1890 of a brotherhood of players called the\n",
      "Players' League, organized for the purpose of securing some of the\n",
      "financial benefits accruing to the managers, as well as for the purpose of\n",
      "abolishing black-listing and other supposed abuses. The Players' League\n",
      "proved not sufficiently strong for the task, and fell to pieces. For some\n",
      "years the National League consisted of twelve clubs organized as stock\n",
      "companies, representing cities as far apart as Boston and St Louis, but in\n",
      "1900 the number was reduced to eight, namely, Boston, Brooklyn, Chicago,\n",
      "Cincinnati, New York, Pittsburg, Philadelphia and St Louis. Certain\n",
      "aggressive and dissatisfied elements took advantage of this change to\n",
      "organize a second great professional association under the presidency of\n",
      "B. B. Johnson, the \"American League,\" of eight clubs, six of them in cities\n",
      "where the National League was already represented. Most of the clubs of\n",
      "both leagues flourish financially, as also do the many minor associations\n",
      "which control the clubs of the different sections of the country, among\n",
      "which are the Eastern League, the American Association, Western League,\n",
      "Southern Association, New England League, Pacific League and the different\n",
      "state leagues. Professional base-ball has not been free from certain\n",
      "objectionable elements, of which the unnecessary and rowdyish fault-finding\n",
      "with the umpires has been the most evident, but the authorities of the\n",
      "different leagues have lately succeeded, by strenuous legislation, in\n",
      "abating these. Of authorities on base-ball, Henry Chadwick (d. 1908) is the\n",
      "best known.\n",
      "\n",
      "Answer: {'score': 0.5971705317497253, 'start': 125, 'end': 144, 'answer': 'Canada and in Japan'}\n",
      "\n",
      "\n",
      "\n",
      "Top Document 3 Cos_Sim 0.487:\n",
      "\n",
      "The following is a general description of the field and of the manner in\n",
      "which the game is played, but as the game has become highly complicated,\n",
      "situations may arise in playing in which general statements do not strictly\n",
      "hold. Any smooth, level field about 150 yds. long and 100 yds. broad will\n",
      "serve for a base-ball ground. Upon this field is marked out with white\n",
      "chalk a square, commonly called the diamond, smooth, like a cricket pitch,\n",
      "the sides of which measure 30 yds. each, and the nearest corner of which is\n",
      "distant about 30 yds. from the limit of the field. This corner is marked\n",
      "with a white plate, called the home-base or plate, five-sided in shape, two\n",
      "of the sides being 1 ft. long and that towards the pitcher 17 in. At the\n",
      "other three corners and attached to pegs are white canvas bags 15 in.\n",
      "square filled with some soft material, and called, beginning at the right\n",
      "as one looks towards the field, first-base, second-base and third-base\n",
      "respectively. The lines from home-base to first, and from home to third are\n",
      "indefinitely prolonged and called foul-lines. The game is played by two\n",
      "sides of nine men each, one of these taking its turn at the bat while the\n",
      "other is in the field endeavouring, as provided by certain rules, to put\n",
      "out the side at bat. Each side has nine turns, or innings, at bat, unless\n",
      "the side last at bat does not need its ninth innings in order to win; a tie\n",
      "at the end of the ninth innings makes additional innings necessary. A full\n",
      "game usually takes from 1Â½ to 2 hrs. to play. Three batsmen are put out in\n",
      "each innings, and the side scoring the greatest number of runs (complete\n",
      "encircling of the bases without being put out) wins. A runner who is not\n",
      "put out but fails to reach home-base does not score a run, but is \"left on\n",
      "base.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: {'score': 0.11810270696878433, 'start': 1202, 'end': 1214, 'answer': 'in the field'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieve_relevant_documents(finetuned_sbert_model,\n",
    "    'Where do you usually play amateur base ball?', banks_to_bassoon_documents, banks_to_bassoon_embeddings,\n",
    "    qa=qa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-disease",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-darwin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
